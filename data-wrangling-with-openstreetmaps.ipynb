{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with OpenStreetMap and MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenStreetMap is a community built free editable map of the world, inspired by the success of Wikipedia where crowdsourced data is open and free from proprietary restricted use. We see some examples of its use by Craigslist and Foursquare, as an open source alternative to Google Maps.\n",
    "\n",
    "http://www.openstreetmap.org\n",
    "\n",
    "Users can map things such as polylines of roads, draw polygons of buildings or areas of interest, or insert nodes for landmarks. These map elements can be further tagged with details such as street addresses or amenity type. Map data is stored in an XML format. More details about the OSM XML can be found here:\n",
    "\n",
    "http://wiki.openstreetmap.org/wiki/OSM_XML\n",
    "\n",
    "Some highlights of the OSM XML format relevent to this project are:\n",
    "- OSM XML is list of instances of data primatives (nodes, ways, and relations) found within a given bounds\n",
    "- nodes represent dimensionless points on the map\n",
    "- ways contain node references to form either a polyline or polygon on the map\n",
    "- nodes and ways both contain children tag elements that represent key value pairs of descriptive information about a given node or way\n",
    "\n",
    "As with any user generated content, there is likely going to be dirty data. In this project I'll attempt to do some auditing, cleaning, and data summarizing tasks with Python and MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Map Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I chose to ~50MB from the Cupertino, West San Jose Area. I grew up in Cupertino and lived through the tech sprawl of Apple and the Asian/Indian gentrification of the area. I figured that my familiarity with the area and intrinsic interest in my hometown makes it a good candidate for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"425\" height=\"350\" frameborder=\"0\" scrolling=\"no\" marginheight=\"0\" marginwidth=\"0\" src=\"http://www.openstreetmap.org/export/embed.html?bbox=-122.1165%2C37.2571%2C-121.9060%2C37.3636&amp;layer=mapnik\"></iframe><br/><small><a href=\"http://www.openstreetmap.org/#map=12/37.3105/-122.0135\" target=\"_blank\">View Larger Map</a></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"425\" height=\"350\" frameborder=\"0\" scrolling=\"no\" marginheight=\"0\" marginwidth=\"0\" src=\"http://www.openstreetmap.org/export/embed.html?bbox=-122.1165%2C37.2571%2C-121.9060%2C37.3636&amp;layer=mapnik\"></iframe><br/><small><a href=\"http://www.openstreetmap.org/#map=12/37.3105/-122.0135\" target=\"_blank\">View Larger Map</a></small>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the Overpass API to download the OpenStreetMap XML for the corresponding bounding box:\n",
    "\n",
    "http://overpass-api.de/api/map?bbox=-122.1165,37.2571,-121.9060,37.3636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://overpass-api.de/api/map?bbox=-122.1165%2C37.2571%2C-121.9060%2C37.3636'\n",
    "filename = 'cupertino_california.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's Requests library is pretty awesome for downloading this dataset, but it unfortunately keeps all the data in memory by default. Since we're using a much larger dataset, we overcome this limitation with this modified procedure from this stackoverflow post:\n",
    "\n",
    "http://stackoverflow.com/a/16696317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_file(url, local_filename):\n",
    "    # stream = True allows downloading of large files; prevents loading entire file into memory\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                f.flush()\n",
    "                \n",
    "download_file(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OSM XML file downloaded, lets parse through it with ElementTree and count the number of unique element types. Iterative parsing is utilized since the XML is too large to process in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 6644,\n",
      " 'meta': 1,\n",
      " 'nd': 255022,\n",
      " 'node': 214642,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 313,\n",
      " 'tag': 165782,\n",
      " 'way': 28404}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "\n",
    "tags = {}\n",
    "\n",
    "for event, elem in ET.iterparse(filename):\n",
    "    if elem.tag in tags: tags[elem.tag] += 1\n",
    "    else:                tags[elem.tag] = 1\n",
    "\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have built three regular expressions: `lower`, `lower_colon`, and `problemchars`.\n",
    "- `lower`: matches strings containing lower case characters\n",
    "- `lower_colon`: matches strings containing lower case characters and a single colon within the string\n",
    "- `problemchars`: matches characters that cannot be used within keys in MongoDB\n",
    "Here is a sample of OSM XML:\n",
    "```\n",
    "<node id=\"266587529\" lat=\"37.3625767\" lon=\"-122.0251570\" version=\"4\" timestamp=\"2015-03-30T03:17:30Z\" changeset=\"29840833\" uid=\"2793982\" user=\"Dhruv Matani\">\n",
    "    <tag k=\"addr:city\" v=\"Sunnyvale\"/>\n",
    "    <tag k=\"addr:housenumber\" v=\"725\"/>\n",
    "    <tag k=\"addr:postcode\" v=\"94086\"/>\n",
    "    <tag k=\"addr:state\" v=\"California\"/>\n",
    "    <tag k=\"addr:street\" v=\"South Fair Oaks Avenue\"/>\n",
    "    <tag k=\"amenity\" v=\"restaurant\"/>\n",
    "    <tag k=\"cuisine\" v=\"indian\"/>\n",
    "    <tag k=\"name\" v=\"Arka\"/>\n",
    "    <tag k=\"opening_hours\" v=\"10am - 2:30pm and 5:00pm - 10:00pm\"/>\n",
    "    <tag k=\"takeaway\" v=\"yes\"/>\n",
    "</node>\n",
    "```\n",
    "Within the node element there are ten `tag` children. The key for half of these children begin with `addr:`. Later in this notebook I will use the `lower_colon` regex to help find these keys so I can build a single `address` document within a larger json document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 78267, 'lower_colon': 83553, 'other': 3962, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(k):\n",
    "                keys['lower'] += 1\n",
    "            elif lower_colon.search(k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    \n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "keys = process_map(filename)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets redefine `process_map` to build a set of unique userid's found within the XML. I will then output the length of this set, representing the number of unique users making edits in the chosen map area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        for e in element:\n",
    "            if 'uid' in e.attrib:\n",
    "                users.add(e.attrib['uid'])\n",
    "\n",
    "    return users\n",
    "\n",
    "users = process_map(filename)\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Street Names**\n",
    "\n",
    "The majority of this project will be devoted to auditing and cleaning street names seen within the OSM XML. Street types used by users in the process of mapping are quite often abbreviated. I will attempt to find these abbreviations and replace them with their full text form. The plan of action is as follows:\n",
    "- Build a regex to match the last token in a string (with an optional '.') as this is typically where you would find the street type in an address\n",
    "- Build a list of expected street types that do not need to be cleaned\n",
    "- Parse through the XML looking for tag elements with `k=\"addr:street\"` attributes\n",
    "- Perform a search using the regex on the value of the v attribute of these elements (the street name string)\n",
    "- Build a dictionary with keys that are matches to the regex (street types) and a set of street names where the particular key was found as the value. This will allow us to determine what needs to be cleaned.\n",
    "- Build a second dictionary that contains a map from an offending street type to a clean street type\n",
    "- Build a second regex that will match these offending street types anywhere in a string\n",
    "- Build a function that will return a clean string using the mapping dictionary and this second regex\n",
    "\n",
    "The first step is to build a regex to match the last token in a string optionally ending with a period. I will also build a list of street types I expect to see in a clean street name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected_street_types = [\"Avenue\", \"Boulevard\", \"Commons\", \"Court\", \"Drive\", \"Lane\", \"Parkway\", \n",
    "                         \"Place\", \"Road\", \"Square\", \"Street\", \"Trail\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `audit_street_type` function will take in the dictionary of street types we are building, a string to audit, a regex to match against that string, and the list of expected street types.\n",
    "\n",
    "The function will search the string for the regex. If there is a match and the match is not in our list of expected street types, add the match as a key to the dictionary and add the string to the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name, regex, expected_street_types):\n",
    "    m = regex.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_street_types:\n",
    "            street_types[street_type].add(street_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `is_street_name` determines if an element contains an attribute `k=\"addr:street\"`. Lets use `is_street_name` as the `tag_filter` when I call the `audit` function to audit street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will define an `audit` function to do the parsing and auditing of the street names.\n",
    "\n",
    "I have defined this function so that it not only audits `tag` elements where `k=\"addr:street\"`, but whichever `tag` elements match the `tag_filter` function. The audit function also takes in a regex and the list of expected matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit(osmfile, regex):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    \n",
    "    # iteratively parse the mapping xml\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        # iterate 'tag' tags within 'node' and 'way' tags\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'], regex, expected_street_types)\n",
    "\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets pretty print the output of `audit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alameda': set(['The Alameda']),\n",
      " 'Ave': set(['Afton Ave',\n",
      "             'Blake Ave',\n",
      "             'Cabrillo Ave',\n",
      "             'N Blaney Ave',\n",
      "             'Saratoga Ave',\n",
      "             'The Alameda Ave']),\n",
      " 'Bascom': set(['S. Bascom']),\n",
      " 'Bellomy': set(['Bellomy']),\n",
      " 'Blvd': set(['De Anza Blvd', 'Stevens Creek Blvd']),\n",
      " 'Circle': set(['Bobolink Circle',\n",
      "                'Calabazas Creek Circle',\n",
      "                'Continental Circle',\n",
      "                'Winchester Circle']),\n",
      " 'Dr': set(['Linwood Dr']),\n",
      " 'East': set(['Vanderbilt Court East']),\n",
      " 'Escuela': set(['Camina Escuela']),\n",
      " 'Franklin': set(['Franklin']),\n",
      " 'Ln': set(['Weyburn Ln']),\n",
      " 'Loop': set(['Infinite Loop']),\n",
      " 'Presada': set(['Paseo Presada']),\n",
      " 'Rd': set(['Bollinger Rd', 'Homestead Rd', 'Saratoga Los Gatos Rd']),\n",
      " 'Real': set(['E El Camino Real', 'East El Camino Real', 'El Camino Real']),\n",
      " 'Row': set(['Santana Row']),\n",
      " 'St': set(['Monroe St']),\n",
      " 'Terrace': set(['Avon Terrace',\n",
      "                 'Avoset Terrace',\n",
      "                 'Devona Terrace',\n",
      "                 'Hobart Terrace',\n",
      "                 'Hogarth Terrace',\n",
      "                 'Lautrec Terrace',\n",
      "                 'Lessing Terrace',\n",
      "                 'Manet Terrace',\n",
      "                 'Oak Point Terrace',\n",
      "                 'Panache Terrace',\n",
      "                 'Pennyroyal Terrace',\n",
      "                 'Pine Pass Terrace',\n",
      "                 'Pistachio Terrace',\n",
      "                 'Pumpkin Terrace',\n",
      "                 'Pyracantha Terrace',\n",
      "                 'Reston Terrace',\n",
      "                 'Riorden Terrace',\n",
      "                 'Springfield Terrace',\n",
      "                 'Wilmington Terrace',\n",
      "                 'Windsor Terrace',\n",
      "                 'Wright Terrace',\n",
      "                 'Yellowstone Terrace']),\n",
      " 'Way': set(['Allison Way',\n",
      "             'Anaconda Way',\n",
      "             'Barnsley Way',\n",
      "             'Belfry Way',\n",
      "             'Belleville Way',\n",
      "             'Bellingham Way',\n",
      "             'Berwick Way',\n",
      "             'Big Basin Way',\n",
      "             'Blanchard Way',\n",
      "             'Bonneville Way',\n",
      "             'Brahms Way',\n",
      "             'Carlisle Way',\n",
      "             'Cheshire Way',\n",
      "             \"Coeur D'Alene Way\",\n",
      "             'Colinton Way',\n",
      "             'Connemara Way',\n",
      "             'Dartshire Way',\n",
      "             'Devonshire Way',\n",
      "             'Dorset Way',\n",
      "             'Dublin Way',\n",
      "             'Duncardine Way',\n",
      "             'Dunholme Way',\n",
      "             'Dunnock Way',\n",
      "             'Durshire Way',\n",
      "             'Edmonds Way',\n",
      "             'Enderby Way',\n",
      "             'Fife Way',\n",
      "             'Firebird Way',\n",
      "             'Flamingo Way',\n",
      "             'Flicker Way',\n",
      "             'Flin Way',\n",
      "             'Golden Way',\n",
      "             'Harney Way',\n",
      "             'Humewick Way',\n",
      "             'Kingfisher Way',\n",
      "             'Lennox Way',\n",
      "             'Locksunart Way',\n",
      "             'Longfellow Way',\n",
      "             'Mallard Way',\n",
      "             'Miette Way',\n",
      "             'Mitty Way',\n",
      "             'Nandina Way',\n",
      "             'Nelson Way',\n",
      "             'Prince Edward Way',\n",
      "             'Pyrus Way',\n",
      "             'Radcliff Way',\n",
      "             'Revelstoke Way',\n",
      "             'Tangerine Way',\n",
      "             'Tartarian Way',\n",
      "             'Ward Way',\n",
      "             'Zinfandel Way']),\n",
      " 'West': set(['Vanderbilt Court West']),\n",
      " 'Winchester': set(['Winchester'])}\n"
     ]
    }
   ],
   "source": [
    "street_types = audit(filename, street_type_re)\n",
    "\n",
    "pprint.pprint(dict(street_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have a list of some abbreviated street types (as well as locations without street types). This is by no means a comprehensive list of all of the abbreviated street types used within the XML as all of these matches occur only as the last token at the end of a street name, but it is a very good first swipe at the problem.\n",
    "\n",
    "To replace these abbreviated street types, I will define an update function that takes a string to update, a mapping dictionary, and a regex to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping, regex):\n",
    "    m = regex.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping:\n",
    "            name = re.sub(regex, mapping[street_type], name)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the results of `audit`, I will build a dictionary to map abbreviations to their full, clean representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_type_mapping = {'Ave'  : 'Avenue',\n",
    "                       'Blvd' : 'Boulevard',\n",
    "                       'Dr'   : 'Drive',\n",
    "                       'Ln'   : 'Lane',\n",
    "                       'Pkwy' : 'Parkway',\n",
    "                       'Rd'   : 'Road',\n",
    "                       'St'   : 'Street'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now want to replace the keys of the map anywhere in the string. I'll build a new regex to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The pipe will cause the regex to search for any of the keys, lazily matching the first it finds\n",
    "street_type_re  = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works, I will traverse the `street_types` dictionary from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Camino Real => El Camino Real\n",
      "E El Camino Real => E El Camino Real\n",
      "East El Camino Real => East El Camino Real\n",
      "S. Bascom => S. Bascom\n",
      "Bellomy => Bellomy\n",
      "Winchester => Winchester\n",
      "Weyburn Ln => Weyburn Lane\n",
      "Linwood Dr => Linwood Drive\n",
      "Franklin => Franklin\n",
      "Monroe St => Monroe Street\n",
      "Bollinger Rd => Bollinger Road\n",
      "Saratoga Los Gatos Rd => Saratoga Los Gatos Road\n",
      "Homestead Rd => Homestead Road\n",
      "Vanderbilt Court East => Vanderbilt Court East\n",
      "Riorden Terrace => Riorden Terrace\n",
      "Yellowstone Terrace => Yellowstone Terrace\n",
      "Springfield Terrace => Springfield Terrace\n",
      "Oak Point Terrace => Oak Point Terrace\n",
      "Windsor Terrace => Windsor Terrace\n",
      "Lessing Terrace => Lessing Terrace\n",
      "Avon Terrace => Avon Terrace\n",
      "Hobart Terrace => Hobart Terrace\n",
      "Wright Terrace => Wright Terrace\n",
      "Hogarth Terrace => Hogarth Terrace\n",
      "Manet Terrace => Manet Terrace\n",
      "Pyracantha Terrace => Pyracantha Terrace\n",
      "Pistachio Terrace => Pistachio Terrace\n",
      "Wilmington Terrace => Wilmington Terrace\n",
      "Avoset Terrace => Avoset Terrace\n",
      "Lautrec Terrace => Lautrec Terrace\n",
      "Devona Terrace => Devona Terrace\n",
      "Pennyroyal Terrace => Pennyroyal Terrace\n",
      "Panache Terrace => Panache Terrace\n",
      "Pumpkin Terrace => Pumpkin Terrace\n",
      "Reston Terrace => Reston Terrace\n",
      "Pine Pass Terrace => Pine Pass Terrace\n",
      "Firebird Way => Firebird Way\n",
      "Dublin Way => Dublin Way\n",
      "Flicker Way => Flicker Way\n",
      "Anaconda Way => Anaconda Way\n",
      "Tartarian Way => Tartarian Way\n",
      "Barnsley Way => Barnsley Way\n",
      "Tangerine Way => Tangerine Way\n",
      "Blanchard Way => Blanchard Way\n",
      "Fife Way => Fife Way\n",
      "Flamingo Way => Flamingo Way\n",
      "Edmonds Way => Edmonds Way\n",
      "Locksunart Way => Locksunart Way\n",
      "Revelstoke Way => Revelstoke Way\n",
      "Enderby Way => Enderby Way\n",
      "Cheshire Way => Cheshire Way\n",
      "Colinton Way => Colinton Way\n",
      "Dorset Way => Dorset Way\n",
      "Berwick Way => Berwick Way\n",
      "Radcliff Way => Radcliff Way\n",
      "Brahms Way => Brahms Way\n",
      "Dunholme Way => Dunholme Way\n",
      "Durshire Way => Durshire Way\n",
      "Longfellow Way => Longfellow Way\n",
      "Nandina Way => Nandina Way\n",
      "Dunnock Way => Dunnock Way\n",
      "Carlisle Way => Carlisle Way\n",
      "Mitty Way => Mitty Way\n",
      "Harney Way => Harney Way\n",
      "Devonshire Way => Devonshire Way\n",
      "Belfry Way => Belfry Way\n",
      "Prince Edward Way => Prince Edward Way\n",
      "Pyrus Way => Pyrus Way\n",
      "Golden Way => Golden Way\n",
      "Ward Way => Ward Way\n",
      "Kingfisher Way => Kingfisher Way\n",
      "Connemara Way => Connemara Way\n",
      "Allison Way => Allison Way\n",
      "Flin Way => Flin Way\n",
      "Nelson Way => Nelson Way\n",
      "Bellingham Way => Bellingham Way\n",
      "Mallard Way => Mallard Way\n",
      "Humewick Way => Humewick Way\n",
      "Big Basin Way => Big Basin Way\n",
      "Coeur D'Alene Way => Coeur D'Alene Way\n",
      "Belleville Way => Belleville Way\n",
      "Duncardine Way => Duncardine Way\n",
      "Bonneville Way => Bonneville Way\n",
      "Miette Way => Miette Way\n",
      "Zinfandel Way => Zinfandel Way\n",
      "Lennox Way => Lennox Way\n",
      "Dartshire Way => Dartshire Way\n",
      "Vanderbilt Court West => Vanderbilt Court West\n",
      "De Anza Blvd => De Anza Boulevard\n",
      "Stevens Creek Blvd => Stevens Creek Boulevard\n",
      "Blake Ave => Blake Avenue\n",
      "The Alameda Ave => The Alameda Avenue\n",
      "Saratoga Ave => Saratoga Avenue\n",
      "Afton Ave => Afton Avenue\n",
      "N Blaney Ave => N Blaney Avenue\n",
      "Cabrillo Ave => Cabrillo Avenue\n",
      "Winchester Circle => Winchester Circle\n",
      "Calabazas Creek Circle => Calabazas Creek Circle\n",
      "Continental Circle => Continental Circle\n",
      "Bobolink Circle => Bobolink Circle\n",
      "Santana Row => Santana Row\n",
      "The Alameda => The Alameda\n",
      "Paseo Presada => Paseo Presada\n",
      "Infinite Loop => Infinite Loop\n",
      "Camina Escuela => Camina Escuela\n"
     ]
    }
   ],
   "source": [
    "for street_type, ways in street_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, street_type_mapping, street_type_re)\n",
    "        print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the abbreviated street types updated as expected.\n",
    "\n",
    "Upon closer inspection, I see another problem: cardinal directions. North, South, East, and West appear to be universally abbreviated. Lets apply similar techniques to replace these abbreviated cardinal directions.\n",
    "\n",
    "First, I will create a new regex matching the set of characters NSEW at the beginning of a string, followed by an optional period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_type_pre = re.compile(r'^[NSEW]\\b\\.?', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To audit, I can use the same function with this new regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E': set(['E El Camino Real']),\n",
      " 'N': set(['N Blaney Ave']),\n",
      " 'S.': set(['S. Bascom'])}\n"
     ]
    }
   ],
   "source": [
    "cardinal_directions = audit(filename, street_type_pre)\n",
    "\n",
    "pprint.pprint(dict(cardinal_directions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we found E, N, S, W, and W. at beginning of the street names. Informative, but I can just create an exhaustive mapping for this issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardinal_mapping = {'E'  : 'East',\n",
    "                    'E.' : 'East',\n",
    "                    'N'  : 'North',\n",
    "                    'N.' : 'North',\n",
    "                    'S'  : 'South',\n",
    "                    'S.' : 'South',\n",
    "                    'W'  : 'West',\n",
    "                    'W.' : 'West'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will traverse the `cardinal_directions` dictionary and apply the updates for both street type and cardinal direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E El Camino Real => E El Camino Real => East El Camino Real\n",
      "S. Bascom => S. Bascom => South Bascom\n",
      "N Blaney Ave => N Blaney Avenue => North Blaney Avenue\n"
     ]
    }
   ],
   "source": [
    "for cardinal_direction, ways in cardinal_directions.iteritems():\n",
    "    if cardinal_direction in cardinal_mapping:\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, street_type_mapping, street_type_re)\n",
    "            best_name   = update_name(better_name, cardinal_mapping, street_type_pre)\n",
    "            print name, \"=>\", better_name, \"=>\", best_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the XML data into MongoDB, I will have to transform the data into json documents structured like this:\n",
    "```\n",
    "{\n",
    "    \"id\": \"2406124091\",\n",
    "    \"type: \"node\",\n",
    "    \"visible\":\"true\",\n",
    "    \"created\": {\n",
    "                  \"version\":\"2\",\n",
    "                  \"changeset\":\"17206049\",\n",
    "                  \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "                  \"user\":\"linuxUser16\",\n",
    "                  \"uid\":\"1219059\"\n",
    "               },\n",
    "    \"pos\": [41.9757030, -87.6921867],\n",
    "    \"address\": {\n",
    "                  \"housenumber\": \"5157\",\n",
    "                  \"postcode\": \"60625\",\n",
    "                  \"street\": \"North Lincoln Ave\"\n",
    "               },\n",
    "    \"amenity\": \"restaurant\",\n",
    "    \"cuisine\": \"mexican\",\n",
    "    \"name\": \"La Cabana De Don Luis\",\n",
    "    \"phone\": \"1 (773)-271-5176\"\n",
    "}\n",
    "```\n",
    "The transform will follow these rules:\n",
    "- Process only 2 types of top level tags: node and way\n",
    "- All attributes of node and way should be turned into regular key/value pairs, except:\n",
    "  - The following attributes should be added under a key `created: version, changeset, timestamp, user, uid`\n",
    "  - Attributes for latitude and longitude should be added to a pos array, for use in geospacial indexing. Make sure the values inside pos array are floats and not strings.\n",
    "- If second level `tag` \"k\" value contains problematic characters, it should be ignored\n",
    "- If second level `tag` \"k\" value starts with \"addr:\", it should be added to a dictionary address\n",
    "- If second level `tag` \"k\" value does not start with \"addr:\", but contains \":\", you can process it same as any other tag.\n",
    "- If there is a second \":\" that separates the type/direction of a street, the tag should be ignored, for example:\n",
    "```\n",
    "<tag k=\"addr:housenumber\" v=\"5158\"/>\n",
    "<tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>\n",
    "<tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "<tag k=\"addr:street:prefix\" v=\"North\"/>\n",
    "<tag k=\"addr:street:type\" v=\"Avenue\"/>\n",
    "<tag k=\"amenity\" v=\"pharmacy\"/>\n",
    "```\n",
    "should be turned into:\n",
    "```\n",
    "{\n",
    "    \"address\": {\n",
    "                   \"housenumber\": 5158,\n",
    "                   \"street\": \"North Lincoln Avenue\"\n",
    "               },\n",
    "    \"amenity\": \"pharmacy\"\n",
    "}\n",
    "```\n",
    "For \"way\" specifically:\n",
    "```\n",
    "<nd ref=\"305896090\"/>\n",
    "<nd ref=\"1719825889\"/>\n",
    "```\n",
    "should be turned into:\n",
    "```\n",
    "{\n",
    "    \"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "}\n",
    "```\n",
    "To do this transformation, lets define a function `shape_element` that processes an element. Within this function I will use the update function with the regexes and mapping dictionaries defined above to clean street addresses. Additionally, I will store timestamp as a Python `datetime` rather than as a string. The format of the timestamp can be found here:\n",
    "\n",
    "http://overpass-api.de/output_formats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}    \n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['type'] = element.tag\n",
    "        \n",
    "        # Parse attributes\n",
    "        for attrib in element.attrib:\n",
    "            # Data creation details\n",
    "            if attrib in CREATED:\n",
    "                if 'created' not in node:\n",
    "                    node['created'] = {}\n",
    "                node['created'][attrib] = element.get(attrib)\n",
    "            # Parse position\n",
    "            if attrib in ['lat', 'lon']:\n",
    "                lat = float(element.attrib.get('lat'))\n",
    "                lon = float(element.attrib.get('lon'))\n",
    "                node['pos'] = [lat, lon]\n",
    "            # Parse the rest of attributes\n",
    "            else:\n",
    "                node[attrib] = element.attrib.get(attrib)\n",
    "            \n",
    "        # Process tags\n",
    "        for tag in element.iter('tag'):\n",
    "            key = tag.attrib['k']\n",
    "            value = tag.attrib['v']\n",
    "            if not problemchars.search(key):\n",
    "                if key[:5] == 'addr:':\n",
    "                    if 'address' not in node:\n",
    "                        node['address'] = {}\n",
    "                    if ':' not in key[5:]:\n",
    "                        node['address'][key[5:]] = value\n",
    "        \n",
    "        # Process nodes\n",
    "        for nd in element.iter('nd'):\n",
    "            if 'node_refs' not in node:\n",
    "                node['node_refs'] = []\n",
    "            node['node_refs'].append(nd.attrib['ref'])\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now parse the XML, shape the elements, and write to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)    \n",
    "    with open(file_out, \"wb\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "\n",
    "process_map(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the size of the files we worked with and generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The downloaded file is 50.66996 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print \"The downloaded file is {} MB\".format(os.path.getsize(filename)/1.0e6) # convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The json file is 79.072265 MB\n"
     ]
    }
   ],
   "source": [
    "print \"The json file is {} MB\".format(os.path.getsize(filename + \".json\")/1.0e6) # convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plenty of Street Addresses**\n",
    "\n",
    "Besides dirty data within the `addr:street` field, we're working with a sizeable amount of data on street addresses. Here I will count the total number of nodes and ways that contain a tag child with `k=\"addr:street\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8958"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_file = open(filename, \"r\")\n",
    "address_count = 0\n",
    "\n",
    "for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"): \n",
    "            if is_street_name(tag):\n",
    "                address_count += 1\n",
    "\n",
    "address_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of locations on the map that has their street addresses tagged. It looks like OpenStreetMap's community has collected a good amount of data for this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to execute mongod to run MongoDB. There are plenty of guides to do this. On OS X, if you have `mongodb` installed via homebrew, homebrew actually has a handy `brew services` command.\n",
    "\n",
    "To start mongodb:\n",
    "\n",
    "    brew services start mongodb\n",
    "\n",
    "To stop mongodb if it's already running:\n",
    "\n",
    "    brew services stop mongodb\n",
    "\n",
    "Alternatively, if you have MongoDB installed and configured already we can run a subprocess for the duration of the python session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "import subprocess\n",
    "\n",
    "# The os.setsid() is passed in the argument preexec_fn so\n",
    "# it's run after the fork() and before  exec() to run the shell.\n",
    "pro = subprocess.Popen(\"mongod\", preexec_fn = os.setsid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, connect to the database with `pymongo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "db_name = 'openstreetmap'\n",
    "\n",
    "# Connect to Mongo DB\n",
    "client = MongoClient('localhost:27017')\n",
    "# Database 'openstreetmap' will be created if it does not exist.\n",
    "db = client[db_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then just import the dataset with `mongoimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping collection: cupertino_california\n",
      "Executing: mongoimport -h 127.0.0.1:27017 --db openstreetmap --collection cupertino_california --file /Users/James/Dropbox/Projects/da/data-wrangling-with-openstreetmap-and-mongodb/cupertino_california.osm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build mongoimport command\n",
    "collection = filename[:filename.find(\".\")]\n",
    "working_directory = \"/Users/James/Dropbox/Projects/da/data-wrangling-with-openstreetmap-and-mongodb/\"\n",
    "json_file = filename + \".json\"\n",
    "\n",
    "mongoimport_cmd = \"mongoimport -h 127.0.0.1:27017 \" + \\\n",
    "                  \"--db \" + db_name + \\\n",
    "                  \" --collection \" + collection + \\\n",
    "                  \" --file \" + working_directory + json_file\n",
    "\n",
    "# Before importing, drop collection if it exists (i.e. a re-run)\n",
    "if collection in db.collection_names():\n",
    "    print \"Dropping collection: \" + collection\n",
    "    db[collection].drop()\n",
    "    \n",
    "# Execute the command\n",
    "print \"Executing: \" + mongoimport_cmd\n",
    "subprocess.call(mongoimport_cmd.split())\n",
    "subprocess.call('ls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing, get the collection from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cupertino_california = db[collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where the fun stuff starts. Now that we have a audited and cleaned up collection, we can query for a bunch of interesting statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243046"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cupertino_california.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Unique Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cupertino_california.distinct('created.user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Nodes and Ways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': u'way', u'count': 28404}, {u'_id': u'node', u'count': 214642}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cupertino_california.aggregate({'$group': {'_id': '$type',\n",
    "                                           'count': {'$sum' : 1}}})['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**number of chosen type of nodes, like cafes, shops etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': u'n76', u'count': 66090}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cupertino_california.aggregate([{\"$group\" : {\"_id\" : \"$created.user\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "                           {\"$sort\" : {\"count\" : -1}}, \\\n",
    "                           {\"$limit\" : 1}])['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('55e6593bc3e84ed4376165c8'),\n",
      " u'changeset': u'32109704',\n",
      " u'created': {u'changeset': u'32109704',\n",
      "              u'timestamp': u'2015-06-21T05:07:49Z',\n",
      "              u'uid': u'33757',\n",
      "              u'user': u'Minh Nguyen',\n",
      "              u'version': u'19'},\n",
      " u'id': u'282814553',\n",
      " u'pos': [37.3520588, -121.93721],\n",
      " u'timestamp': u'2015-06-21T05:07:49Z',\n",
      " u'type': u'node',\n",
      " u'uid': u'33757',\n",
      " u'user': u'Minh Nguyen',\n",
      " u'version': u'19'}\n"
     ]
    }
   ],
   "source": [
    "node_id = cupertino_california.aggregate([{\"$unwind\" : \"$node_refs\"}, \\\n",
    "                                     {\"$group\" : {\"_id\" : \"$node_refs\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "                                     {\"$sort\" : {\"count\" : -1}}, \\\n",
    "                                     {\"$limit\" : 1}])['result'][0]['_id']\n",
    "\n",
    "pprint.pprint(cupertino_california.find({\"id\" : node_id})[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9095"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cupertino_california.find({\"address.street\" : {\"$exists\" : 1}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': u'94087', u'count': 226},\n",
       " {u'_id': u'95070', u'count': 225},\n",
       " {u'_id': u'95051', u'count': 127},\n",
       " {u'_id': u'95014', u'count': 106},\n",
       " {u'_id': u'95129', u'count': 86},\n",
       " {u'_id': u'95126', u'count': 45},\n",
       " {u'_id': u'95008', u'count': 41},\n",
       " {u'_id': u'95050', u'count': 28},\n",
       " {u'_id': u'95125', u'count': 13},\n",
       " {u'_id': u'94086', u'count': 12},\n",
       " {u'_id': u'95117', u'count': 9},\n",
       " {u'_id': u'95128', u'count': 8},\n",
       " {u'_id': u'94024', u'count': 5},\n",
       " {u'_id': u'95124', u'count': 4},\n",
       " {u'_id': u'94040', u'count': 3},\n",
       " {u'_id': u'95032', u'count': 3},\n",
       " {u'_id': u'94087-2248', u'count': 1},\n",
       " {u'_id': u'94087\\u200e', u'count': 1},\n",
       " {u'_id': u'94088-3707', u'count': 1},\n",
       " {u'_id': u'95110', u'count': 1},\n",
       " {u'_id': u'95052', u'count': 1},\n",
       " {u'_id': u'CA 95014', u'count': 1},\n",
       " {u'_id': u'95914', u'count': 1},\n",
       " {u'_id': u'94022', u'count': 1},\n",
       " {u'_id': u'CA 94086', u'count': 1}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cupertino_california.aggregate([{\"$match\" : {\"address.postcode\" : {\"$exists\" : 1}}}, \\\n",
    "                           {\"$group\" : {\"_id\" : \"$address.postcode\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "                           {\"$sort\" : {\"count\" : -1}}])['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': u'Sunnyvale', u'count': 2476},\n",
       " {u'_id': u'Saratoga', u'count': 221},\n",
       " {u'_id': u'Santa Clara', u'count': 142},\n",
       " {u'_id': u'San Jose', u'count': 99},\n",
       " {u'_id': u'Cupertino', u'count': 59},\n",
       " {u'_id': u'Campbell', u'count': 37},\n",
       " {u'_id': u'San Jos\\xe9', u'count': 9},\n",
       " {u'_id': u'Los Altos', u'count': 7},\n",
       " {u'_id': u'Campbelll', u'count': 3},\n",
       " {u'_id': u'Mountain View', u'count': 3},\n",
       " {u'_id': u'cupertino', u'count': 2},\n",
       " {u'_id': u'Santa clara', u'count': 1},\n",
       " {u'_id': u'santa clara', u'count': 1},\n",
       " {u'_id': u'campbell', u'count': 1},\n",
       " {u'_id': u'san jose', u'count': 1},\n",
       " {u'_id': u'Los Gatos', u'count': 1},\n",
       " {u'_id': u'South Mary Avenue', u'count': 1},\n",
       " {u'_id': u'sunnyvale', u'count': 1}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cupertino_california.aggregate([{\"$match\" : {\"address.city\" : {\"$exists\" : 1}}}, \\\n",
    "                           {\"$group\" : {\"_id\" : \"$address.city\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "                           {\"$sort\" : {\"count\" : -1}}])['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
